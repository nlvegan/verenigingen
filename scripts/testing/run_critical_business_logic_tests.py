#!/usr/bin/env python3
"""
Critical Business Logic Test Runner
===================================

Enterprise-grade test orchestration system for validating core business logic integrity
and ensuring deployment readiness of the Verenigingen association management system.

This critical infrastructure component serves as the final quality gate before deployment,
systematically validating that all essential business logic functions correctly and that
no critical functionality has been compromised during development cycles.

Core Mission
-----------
The Critical Business Logic Test Runner addresses the fundamental challenge of maintaining
business logic integrity in a complex association management system where:

- Member lifecycle operations must function flawlessly
- Financial operations require 100% accuracy
- Volunteer management processes are mission-critical
- Integration points with external systems must be reliable
- DocType interdependencies must remain consistent

Strategic Importance
-------------------
**Deployment Gate**: Serves as the primary quality gate for production deployments
**Business Continuity**: Ensures critical association operations remain functional
**Regression Prevention**: Catches breaking changes before they reach production
**Compliance Validation**: Verifies business rule compliance across all operations
**Integration Verification**: Validates external system integration points

Architecture and Design
----------------------

### Test Orchestration Strategy
The runner employs a tiered testing approach:

1. **Core Critical Business Logic**: Fundamental business rule validation
2. **High-Risk DocType Tests**: Tests for DocTypes with complex business logic
3. **Comprehensive DocType Tests**: Full validation of DocType functionality
4. **Core DocType Tests**: Essential DocType operations that must always work

### Execution Framework
- **Sequential Execution**: Tests run sequentially to avoid interference
- **Isolated Environments**: Each test module runs in isolated database state
- **Comprehensive Reporting**: Detailed success/failure tracking with diagnostics
- **Early Termination**: Optional fail-fast mode for rapid feedback

### Quality Metrics
- **Success Rate Tracking**: Percentage of tests passing for trend analysis
- **Module-Level Reporting**: Individual test module success/failure status
- **Failure Analysis**: Detailed failure information for rapid debugging
- **Historical Comparison**: Baseline comparison for regression detection

Test Module Categories
---------------------

### 1. Core Critical Business Logic Tests
**Purpose**: Validate fundamental business operations
**Modules**: `test_critical_business_logic`
**Coverage**: 
- Member lifecycle operations (registration, status changes, termination)
- Financial calculation accuracy (dues, fees, payments)
- Volunteer assignment and management workflows
- Chapter membership and role management

### 2. High-Risk DocType Tests  
**Purpose**: Validate DocTypes with complex business rules
**Modules**: 
- `test_membership_termination_request_critical`
- `test_e_boekhouden_migration_critical`
**Coverage**:
- Complex state transition logic
- Financial integration points
- Data migration and transformation operations
- External system synchronization

### 3. Comprehensive DocType Tests
**Purpose**: Full validation of DocType functionality
**Modules**:
- `test_membership_termination_request`
- `test_e_boekhouden_migration`
**Coverage**:
- Complete DocType method validation
- Edge case handling verification
- Permission and security rule testing
- Integration workflow validation

### 4. Core DocType Tests
**Purpose**: Essential operations that must always function
**Modules**:
- `test_membership`, `test_member`, `test_chapter`
- `test_volunteer`, `test_volunteer_expense`
**Coverage**:
- Basic CRUD operations for all core entities
- Required field validation
- Business rule enforcement
- Relationship integrity checking

Deployment Integration
---------------------

### Pre-Deployment Validation
```bash
# Run as part of deployment pipeline
python scripts/testing/run_critical_business_logic_tests.py

# Check exit code for deployment decision
if [ $? -eq 0 ]; then
    echo "âœ… Deployment approved - all critical tests passed"
    proceed_with_deployment
else
    echo "âŒ Deployment blocked - critical test failures detected"
    abort_deployment
fi
```

### CI/CD Pipeline Integration
```yaml
# GitHub Actions / GitLab CI integration
test_critical_business_logic:
  script:
    - cd /path/to/app
    - python scripts/testing/run_critical_business_logic_tests.py
  when: always
  artifacts:
    reports:
      junit: test_results.xml
```

### Manual Quality Assurance
```bash
# Manual testing before critical deployments
./scripts/testing/run_critical_business_logic_tests.py

# Review detailed output for any concerns
grep -i "failed\|error" test_output.log
```

Error Handling and Recovery
---------------------------

### Failure Detection Strategy
The runner employs comprehensive failure detection:

- **Return Code Analysis**: Captures subprocess return codes for each test module
- **Output Parsing**: Analyzes stdout/stderr for error patterns
- **Exception Handling**: Graceful handling of system-level errors
- **Timeout Management**: Prevents hanging on infinite loops or deadlocks

### Failure Reporting
When tests fail, the runner provides:

```
âŒ verenigingen.doctype.membership.test_membership - FAILED
Return code: 1
STDERR:
AssertionError: Membership calculation failed for member MEMBER-001
Expected: 50.00, Got: 0.00
```

### Recovery Procedures
**Test Environment Reset**: Automatic database cleanup between test modules
**Dependency Resolution**: Ensures required fixtures and data are available
**Configuration Validation**: Verifies test environment configuration
**Resource Cleanup**: Prevents resource leaks between test executions

Performance Characteristics
--------------------------

### Execution Metrics
- **Typical Runtime**: 5-15 minutes for complete test suite
- **Individual Module Time**: 30 seconds to 3 minutes per module
- **Resource Usage**: Moderate CPU and database I/O during execution
- **Memory Requirements**: ~100-500MB depending on test data volume

### Optimization Strategies
- **Parallel Potential**: Architecture supports future parallel execution
- **Incremental Testing**: Can be enhanced to support delta testing
- **Caching Opportunities**: Test data preparation can be optimized
- **Resource Pooling**: Database connections and fixtures can be pooled

### Scalability Considerations
- **Test Suite Growth**: Designed to accommodate new test modules
- **Data Volume Scaling**: Handles increasing test data requirements
- **Infrastructure Scaling**: Compatible with larger test environments
- **Distributed Execution**: Architecture supports distributed test execution

Quality Assurance Features
--------------------------

### Success Rate Monitoring
The runner tracks and reports success rates:
```
ğŸ“Š TEST RESULTS SUMMARY
Total modules tested: 7
Passed: 6
Failed: 1
Success rate: 85.7%
```

### Historical Trend Analysis
- **Baseline Comparison**: Compare current results with historical baselines
- **Regression Detection**: Identify when success rates decline
- **Improvement Tracking**: Monitor test suite reliability improvements
- **Failure Pattern Analysis**: Identify recurring failure patterns

### Quality Gates
- **100% Success Requirement**: Production deployments require 100% test success
- **Acceptable Failure Thresholds**: Development environments may allow some failures
- **Critical vs Non-Critical**: Different thresholds for different test categories
- **Override Mechanisms**: Emergency deployment overrides with proper approval

Maintenance and Evolution
------------------------

### Test Module Management
- **Dynamic Module Loading**: Easy addition of new test modules
- **Categorization System**: Organized test module categories
- **Priority Weighting**: Different priorities for different test types
- **Dependency Management**: Handles test module dependencies

### Configuration Management
```python
# Easily configurable test module lists
CRITICAL_TEST_MODULES = [
    "verenigingen.tests.backend.business_logic.test_critical_business_logic",
    # Add new critical tests here
]

# Customizable for different environments
ENVIRONMENT_SPECIFIC_MODULES = {
    "production": CRITICAL_TEST_MODULES,
    "staging": CRITICAL_TEST_MODULES + EXPERIMENTAL_MODULES,
    "development": ALL_TEST_MODULES
}
```

### Integration Points
- **Test Framework Integration**: Compatible with pytest, unittest, and Frappe test frameworks
- **Reporting System Integration**: Outputs compatible with CI/CD reporting systems
- **Monitoring System Integration**: Metrics can be exported to monitoring dashboards
- **Notification Integration**: Can trigger alerts on test failures

Troubleshooting and Diagnostics
------------------------------

### Common Failure Scenarios

**Database Connection Issues**
```
Error running test module: Database connection failed
Resolution: Check database availability and credentials
```

**Missing Test Dependencies**
```
ModuleNotFoundError: No module named 'test_critical_business_logic'
Resolution: Verify test module paths and app installation
```

**Permission Issues**
```
PermissionError: Access denied to test database
Resolution: Verify Frappe site permissions and user access
```

**Environment Configuration**
```
Configuration error: Missing required environment variables
Resolution: Check environment setup and configuration files
```

### Diagnostic Tools
- **Verbose Mode**: Detailed output for debugging test failures
- **Module Isolation**: Run individual test modules for focused debugging
- **Environment Validation**: Pre-flight checks for environment readiness
- **Resource Monitoring**: Track resource usage during test execution

### Support Procedures
1. **Immediate Response**: Critical test failures trigger immediate investigation
2. **Root Cause Analysis**: Systematic analysis of test failure causes
3. **Fix Verification**: Re-run tests after implementing fixes
4. **Prevention Measures**: Update tests to prevent similar future failures

Business Impact and Value
-------------------------

### Risk Mitigation
- **Production Stability**: Prevents deployment of broken business logic
- **Data Integrity**: Ensures financial calculations remain accurate
- **User Experience**: Prevents functionality regressions that impact users
- **Compliance Assurance**: Validates regulatory and business rule compliance

### Operational Excellence
- **Deployment Confidence**: Provides confidence in deployment safety
- **Quality Metrics**: Objective quality measurements for the application
- **Process Improvement**: Feedback loop for development process enhancement
- **Documentation**: Living documentation of critical system functionality

### Cost Benefit Analysis
- **Prevented Incidents**: Each caught bug prevents potential production incidents
- **Reduced Manual Testing**: Automated validation reduces manual QA overhead
- **Faster Recovery**: Rapid identification of issues enables faster resolution
- **Quality Culture**: Promotes quality-first development practices

This critical test runner represents a cornerstone of the Verenigingen quality assurance
strategy, providing the confidence needed for safe, reliable deployments in a mission-
critical association management environment.
"""

import os
import subprocess
import sys

# Test modules to run
CRITICAL_TEST_MODULES = [
    # Core critical business logic tests
    "verenigingen.tests.backend.business_logic.test_critical_business_logic",
    # High-risk doctype tests
    "verenigingen.verenigingen.doctype.membership_termination_request.test_membership_termination_request_critical",
    "verenigingen.e_boekhouden.doctype.e_boekhouden_migration.test_e_boekhouden_migration_critical",
    # Comprehensive doctype tests
    "verenigingen.verenigingen.doctype.membership_termination_request.test_membership_termination_request",
    "verenigingen.e_boekhouden.doctype.e_boekhouden_migration.test_e_boekhouden_migration",
    # Core doctype tests that should always pass
    "verenigingen.verenigingen.doctype.membership.test_membership",
    "verenigingen.verenigingen.doctype.member.test_member",
    "verenigingen.verenigingen.doctype.chapter.test_chapter",
    "verenigingen.verenigingen.doctype.volunteer.test_volunteer",
    "verenigingen.verenigingen.doctype.volunteer_expense.test_volunteer_expense",
]


def run_test_module(module_name):
    """Run a single test module"""
    print(f"\n{'='*60}")
    print(f"Running tests: {module_name}")
    print(f"{'='*60}")

    cmd = [
        "bench",
        "--site",
        "dev.veganisme.net",
        "run-tests",
        "--app",
        "verenigingen",
        "--module",
        module_name,
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, cwd="/home/frappe/frappe-bench")

        print(f"Return code: {result.returncode}")
        if result.stdout:
            print("STDOUT:")
            print(result.stdout)
        if result.stderr:
            print("STDERR:")
            print(result.stderr)

        return result.returncode == 0

    except Exception as e:
        print(f"Error running test module {module_name}: {e}")
        return False


def main():
    """Run all critical business logic tests"""
    print("ğŸš€ Starting Critical Business Logic Test Suite")
    print(f"Running {len(CRITICAL_TEST_MODULES)} test modules...")

    results = {}
    total_passed = 0
    total_failed = 0

    for module in CRITICAL_TEST_MODULES:
        success = run_test_module(module)
        results[module] = success

        if success:
            total_passed += 1
            print(f"âœ… {module} - PASSED")
        else:
            total_failed += 1
            print(f"âŒ {module} - FAILED")

    print(f"\n{'='*60}")
    print("ğŸ“Š TEST RESULTS SUMMARY")
    print(f"{'='*60}")
    print(f"Total modules tested: {len(CRITICAL_TEST_MODULES)}")
    print(f"Passed: {total_passed}")
    print(f"Failed: {total_failed}")
    print(f"Success rate: {(total_passed / len(CRITICAL_TEST_MODULES)) * 100:.1f}%")

    if total_failed > 0:
        print(f"\nâŒ FAILED MODULES:")
        for module, success in results.items():
            if not success:
                print(f"  - {module}")

    print(f"\n{'='*60}")
    if total_failed == 0:
        print("ğŸ‰ ALL CRITICAL TESTS PASSED!")
        print("âœ… Business logic integrity verified")
        print("âœ… No missing methods detected")
        print("âœ… Core functionality working")
    else:
        print("âš ï¸  SOME TESTS FAILED")
        print("ğŸ” Review failed modules above")
        print("ğŸ› ï¸  Fix issues before deployment")

    return total_failed == 0


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
