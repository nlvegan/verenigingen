#!/usr/bin/env python3
"""
Comprehensive Test Suite for DocType Loader Standardization
==========================================================

This test suite validates the massive standardization of 21 validators to use
the comprehensive DocTypeLoader instead of manual DocType loading.

Test Coverage:
- Core DocType loading functionality (1,049+ DocTypes expected)
- Custom field integration (36+ custom fields expected)
- Multi-app support (9 apps: frappe, erpnext, payments, vereinigingen, banking, crm, hrms, owl_theme, erpnext_expenses)
- Field metadata completeness and accuracy
- Caching performance and correctness
- Child table relationship mapping
- Backward compatibility with legacy validators
- Performance regression detection

This uses realistic data from the actual filesystem rather than mocks,
ensuring tests validate real-world functionality.
"""

import json
import time
import unittest
from pathlib import Path
from typing import Dict, Set, List
from unittest.mock import patch

# Import the DocType loader
from doctype_loader import DocTypeLoader, DocTypeMetadata, FieldMetadata, LoadingStats


class TestDocTypeLoaderCore(unittest.TestCase):
    """Core functionality tests for the DocType loader"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment once for all tests"""
        cls.bench_path = "/home/frappe/frappe-bench"
        cls.loader = DocTypeLoader(cls.bench_path, verbose=True)
        
        # Load DocTypes once for performance
        cls.doctypes = cls.loader.get_doctypes()
        cls.stats = cls.loader.get_loading_stats()
        
    def test_doctype_count_meets_expectations(self):
        """Test that we load the expected number of DocTypes"""
        # Based on your specification: 1,049 DocTypes + 36 custom fields
        min_expected_doctypes = 1000  # Allow some variance for environment differences
        max_expected_doctypes = 1200  # Upper bound to catch unexpected inflation
        
        actual_count = len(self.doctypes)
        
        self.assertGreaterEqual(
            actual_count, 
            min_expected_doctypes,
            f"Expected at least {min_expected_doctypes} DocTypes, got {actual_count}. "
            f"This suggests the loader is not finding all installed apps or DocTypes."
        )
        
        self.assertLessEqual(
            actual_count,
            max_expected_doctypes,
            f"Expected at most {max_expected_doctypes} DocTypes, got {actual_count}. "
            f"This suggests unexpected DocTypes are being loaded."
        )
        
        print(f"âœ… DocType count validation: {actual_count} DocTypes loaded")
    
    def test_custom_fields_integration(self):
        """Test that custom fields are properly integrated"""
        min_expected_custom_fields = 30  # Based on specification: 36+ custom fields
        max_expected_custom_fields = 100  # Reasonable upper bound
        
        actual_custom_fields = self.stats.custom_fields
        
        self.assertGreaterEqual(
            actual_custom_fields,
            min_expected_custom_fields,
            f"Expected at least {min_expected_custom_fields} custom fields, got {actual_custom_fields}"
        )
        
        self.assertLessEqual(
            actual_custom_fields,
            max_expected_custom_fields,
            f"Expected at most {max_expected_custom_fields} custom fields, got {actual_custom_fields}"
        )
        
        print(f"âœ… Custom fields validation: {actual_custom_fields} custom fields loaded")
    
    def test_expected_apps_coverage(self):
        """Test that all expected apps are covered"""
        expected_apps = {
            'frappe', 'erpnext', 'payments', 'verenigingen', 
            'banking', 'crm', 'hrms', 'owl_theme', 'erpnext_expenses'
        }
        
        actual_apps = self.stats.apps_scanned
        
        # Check core apps that should always be present
        core_apps = {'frappe', 'erpnext', 'verenigingen'}
        missing_core_apps = core_apps - actual_apps
        
        self.assertEqual(
            len(missing_core_apps), 0,
            f"Missing core apps: {missing_core_apps}. These are required for the application to function."
        )
        
        # Check coverage of expected apps (allow some flexibility for environment differences)
        found_expected_apps = expected_apps & actual_apps
        coverage_ratio = len(found_expected_apps) / len(expected_apps)
        
        self.assertGreaterEqual(
            coverage_ratio, 0.6,  # At least 60% of expected apps should be present
            f"Low app coverage: {coverage_ratio:.1%}. Found {found_expected_apps}, expected {expected_apps}"
        )
        
        print(f"âœ… App coverage validation: {len(actual_apps)} apps scanned, "
              f"{len(found_expected_apps)}/{len(expected_apps)} expected apps found")
    
    def test_core_doctype_presence(self):
        """Test that critical DocTypes are present"""
        critical_doctypes = [
            'User', 'Role', 'DocType', 'DocField', 'Custom Field',  # Frappe core
            'Sales Invoice', 'Customer', 'Item', 'Company',  # ERPNext core
            'Member', 'Chapter', 'Volunteer', 'SEPA Mandate',  # Verenigingen core
        ]
        
        missing_doctypes = []
        for doctype in critical_doctypes:
            if doctype not in self.doctypes:
                missing_doctypes.append(doctype)
        
        self.assertEqual(
            len(missing_doctypes), 0,
            f"Missing critical DocTypes: {missing_doctypes}. This suggests incomplete loading."
        )
        
        print(f"âœ… Critical DocTypes validation: All {len(critical_doctypes)} critical DocTypes found")
    
    def test_field_metadata_completeness(self):
        """Test that field metadata is complete and accurate"""
        # Test a known DocType with expected fields
        test_doctype = 'User'
        self.assertIn(test_doctype, self.doctypes, f"Test DocType '{test_doctype}' not found")
        
        doctype_meta = self.loader.get_doctype(test_doctype)
        self.assertIsNotNone(doctype_meta, f"DocType metadata for '{test_doctype}' is None")
        
        # Check standard fields are present
        field_names = self.loader.get_field_names(test_doctype)
        expected_standard_fields = {'name', 'creation', 'modified', 'owner', 'docstatus'}
        missing_standard_fields = expected_standard_fields - field_names
        
        self.assertEqual(
            len(missing_standard_fields), 0,
            f"Missing standard fields in {test_doctype}: {missing_standard_fields}"
        )
        
        # Check specific User fields
        expected_user_fields = {'email', 'first_name', 'last_name', 'enabled'}
        missing_user_fields = expected_user_fields - field_names
        
        self.assertEqual(
            len(missing_user_fields), 0,
            f"Missing expected User fields: {missing_user_fields}"
        )
        
        print(f"âœ… Field metadata validation: {test_doctype} has {len(field_names)} fields including all expected fields")
    
    def test_child_table_relationships(self):
        """Test that child table relationships are correctly mapped"""
        # Find a DocType with child tables
        child_mapping = self.loader.get_child_table_mapping()
        
        self.assertGreater(
            len(child_mapping), 0,
            "No child table relationships found. Expected some DocTypes to have child tables."
        )
        
        # Validate mapping format and track missing child DocTypes
        missing_child_doctypes = []
        valid_relationships = 0
        
        for parent_field, child_doctype in child_mapping.items():
            self.assertIn('.', parent_field, f"Invalid parent field format: {parent_field}")
            parent_doctype, field_name = parent_field.split('.', 1)
            
            self.assertIn(parent_doctype, self.doctypes, f"Parent DocType {parent_doctype} not found")
            
            # Track missing child DocTypes but don't fail the test
            if child_doctype not in self.doctypes:
                missing_child_doctypes.append(f"{parent_field} -> {child_doctype}")
            else:
                valid_relationships += 1
        
        # Log missing child DocTypes for informational purposes
        if missing_child_doctypes and len(missing_child_doctypes) < 10:  # Only show if reasonable number
            print(f"â„¹ï¸  Missing child DocTypes (expected): {missing_child_doctypes}")
        
        # Ensure we have some valid relationships
        self.assertGreater(
            valid_relationships, 0,
            f"No valid child table relationships found. {len(missing_child_doctypes)} child DocTypes missing."
        )
        
        print(f"âœ… Child table relationships validation: {valid_relationships}/{len(child_mapping)} relationships valid")
    
    def test_field_index_functionality(self):
        """Test that field indexing works correctly"""
        field_index = self.loader.get_field_index()
        
        self.assertGreater(
            len(field_index), 0,
            "No field index entries found. Expected fields to be indexed."
        )
        
        # Test finding DocTypes with common fields
        common_fields = ['name', 'creation', 'modified']
        for field_name in common_fields:
            doctypes_with_field = self.loader.find_doctypes_with_field(field_name)
            self.assertGreater(
                len(doctypes_with_field), 10,  # Most DocTypes should have these standard fields
                f"Too few DocTypes found with standard field '{field_name}': {len(doctypes_with_field)}"
            )
        
        print(f"âœ… Field index validation: {len(field_index)} unique fields indexed across all DocTypes")
    
    def test_caching_performance(self):
        """Test that caching improves performance on subsequent loads"""
        # Clear cache and measure first load
        loader_fresh = DocTypeLoader(self.bench_path, verbose=False)
        
        start_time = time.time()
        doctypes_first = loader_fresh.get_doctypes()
        first_load_time = time.time() - start_time
        
        # Measure cached load
        start_time = time.time()
        doctypes_second = loader_fresh.get_doctypes()
        cached_load_time = time.time() - start_time
        
        # Cached load should be significantly faster
        self.assertLess(
            cached_load_time, first_load_time * 0.1,  # At least 10x faster
            f"Cache not working: first load {first_load_time:.3f}s, cached load {cached_load_time:.3f}s"
        )
        
        # Data should be identical
        self.assertEqual(len(doctypes_first), len(doctypes_second))
        
        print(f"âœ… Caching performance validation: "
              f"First load {first_load_time:.3f}s, cached load {cached_load_time:.3f}s "
              f"({cached_load_time/first_load_time:.1%} of original time)")


class TestValidatorStandardization(unittest.TestCase):
    """Test that validators are properly standardized to use DocTypeLoader"""
    
    @classmethod
    def setUpClass(cls):
        """Set up test environment"""
        cls.validation_dir = Path("/home/frappe/frappe-bench/apps/verenigingen/scripts/validation")
        cls.bench_path = "/home/frappe/frappe-bench"
        
        # List of validators that should be standardized
        cls.standardized_validators = [
            'doctype_field_validator.py',
            'unified_validation_engine.py', 
            'javascript_doctype_field_validator.py',
            'enhanced_field_reference_validator.py',
            'production_ready_validator.py',
            'comprehensive_final_validator.py',
            'intelligent_pattern_validator.py',
            'frappe_api_field_validator.py',
            'bugfix_enhanced_validator.py',
            'doctype_field_validator_modified.py',
            # Add more validators as specified in the standardization
        ]
    
    def test_validators_use_doctype_loader(self):
        """Test that standardized validators import and use DocTypeLoader"""
        
        for validator_file in self.standardized_validators:
            validator_path = self.validation_dir / validator_file
            
            if not validator_path.exists():
                self.skipTest(f"Validator {validator_file} not found - may be renamed or moved")
                continue
            
            with open(validator_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check for DocTypeLoader import
            self.assertIn(
                'from doctype_loader import',
                content,
                f"Validator {validator_file} does not import DocTypeLoader"
            )
            
            # Check for DocTypeLoader instantiation
            doctype_loader_patterns = [
                'DocTypeLoader(',
                'doctype_loader = DocTypeLoader',
                'self.doctype_loader = DocTypeLoader'
            ]
            
            has_loader_usage = any(pattern in content for pattern in doctype_loader_patterns)
            self.assertTrue(
                has_loader_usage,
                f"Validator {validator_file} does not instantiate DocTypeLoader"
            )
            
            print(f"âœ… Validator standardization: {validator_file} uses DocTypeLoader")
    
    def test_validators_load_comprehensive_doctypes(self):
        """Test that validators are loading comprehensive DocType sets"""
        # Import and test a key standardized validator
        try:
            from unified_validation_engine import SpecializedPatternValidator
            
            app_path = "/home/frappe/frappe-bench/apps/verenigingen"
            validator = SpecializedPatternValidator(app_path)
            
            # Check that it loaded a comprehensive set
            doctypes_loaded = len(validator.doctypes)
            self.assertGreater(
                doctypes_loaded, 1000,
                f"Unified validator only loaded {doctypes_loaded} DocTypes, expected 1000+"
            )
            
            print(f"âœ… Comprehensive loading: SpecializedPatternValidator loaded {doctypes_loaded} DocTypes")
            
        except ImportError as e:
            self.skipTest(f"Could not import SpecializedPatternValidator: {e}")
    
    def test_legacy_compatibility_maintained(self):
        """Test that standardized validators maintain legacy compatibility"""
        from doctype_loader import DocTypeLoader
        
        loader = DocTypeLoader(self.bench_path, verbose=False)
        
        # Test legacy format methods
        simple_format = loader.get_doctypes_simple()
        detailed_format = loader.get_doctypes_detailed()
        
        self.assertIsInstance(simple_format, dict)
        self.assertIsInstance(detailed_format, dict)
        
        # Check format consistency
        for doctype_name in simple_format:
            self.assertIn(doctype_name, detailed_format)
            self.assertIsInstance(simple_format[doctype_name], set)  # Field names as set
            self.assertIsInstance(detailed_format[doctype_name], dict)  # Detailed info as dict
        
        print(f"âœ… Legacy compatibility: Simple format has {len(simple_format)} DocTypes, "
              f"detailed format has {len(detailed_format)} DocTypes")


class TestRegressionDetection(unittest.TestCase):
    """Test for regressions in validation functionality"""
    
    def setUp(self):
        """Set up test environment"""
        self.bench_path = "/home/frappe/frappe-bench"
        self.loader = DocTypeLoader(self.bench_path, verbose=False)
    
    def test_no_validators_loading_zero_doctypes(self):
        """Test that no validators are loading 0 DocTypes (path issues)"""
        # This would indicate path configuration problems
        doctypes = self.loader.get_doctypes()
        
        self.assertGreater(
            len(doctypes), 0,
            "DocTypeLoader is loading 0 DocTypes - this indicates a critical path configuration issue"
        )
        
        print(f"âœ… Regression check: DocTypeLoader properly loads {len(doctypes)} DocTypes")
    
    def test_field_validation_accuracy(self):
        """Test field validation using known good and bad examples"""
        # Test with known valid field
        self.assertTrue(
            self.loader.has_field('User', 'email'),
            "Known valid field User.email not found - validation accuracy compromised"
        )
        
        # Test with known invalid field
        self.assertFalse(
            self.loader.has_field('User', 'nonexistent_field_xyz'),
            "Invalid field incorrectly validated as valid - false negative detected"
        )
        
        print("âœ… Field validation accuracy: Known valid/invalid fields correctly identified")
    
    def test_performance_baseline(self):
        """Test that performance hasn't significantly degraded"""
        # Measure loading time
        start_time = time.time()
        doctypes = self.loader.get_doctypes()
        load_time = time.time() - start_time
        
        # Set reasonable performance expectations
        max_acceptable_load_time = 10.0  # 10 seconds max for full load
        
        self.assertLess(
            load_time, max_acceptable_load_time,
            f"DocType loading took {load_time:.2f}s, exceeds maximum of {max_acceptable_load_time}s"
        )
        
        print(f"âœ… Performance baseline: Loaded {len(doctypes)} DocTypes in {load_time:.2f}s")


class TestFunctionalValidation(unittest.TestCase):
    """Test validation functionality with realistic data"""
    
    def setUp(self):
        """Set up test environment"""
        self.app_path = Path("/home/frappe/frappe-bench/apps/verenigingen")
        self.bench_path = "/home/frappe/frappe-bench"
        self.loader = DocTypeLoader(self.bench_path, verbose=False)
    
    def test_real_file_validation(self):
        """Test validation against real Python files in the codebase"""
        # Find a Python file that should have valid field references
        python_files = list(self.app_path.rglob("*.py"))
        test_files = [f for f in python_files if 'doctype' in str(f) and f.name.endswith('.py')]
        
        if not test_files:
            self.skipTest("No suitable Python files found for validation testing")
        
        # Test with the first available file
        test_file = test_files[0]
        
        try:
            from unified_validation_engine import SpecializedPatternValidator
            
            validator = SpecializedPatternValidator(str(self.app_path))
            violations = validator.validate_file(test_file)
            
            # We expect some violations might be found, but the validator shouldn't crash
            self.assertIsInstance(violations, list)
            
            print(f"âœ… Real file validation: Tested {test_file.name}, found {len(violations)} potential issues")
            
        except ImportError:
            self.skipTest("SpecializedPatternValidator not available for testing")
        except Exception as e:
            self.fail(f"Validation failed on real file {test_file}: {e}")
    
    def test_doctype_completeness_validation(self):
        """Test validation of DocType completeness"""
        # Test a critical DocType for completeness
        test_doctype = 'Member'  # Core to the verenigingen app
        
        if test_doctype not in self.loader.get_doctypes():
            self.skipTest(f"Test DocType {test_doctype} not available")
        
        issues = self.loader.validate_doctype_completeness(test_doctype)
        
        # A complete DocType should have minimal issues
        critical_issues = [issue for issue in issues if 'Missing' in issue]
        self.assertEqual(
            len(critical_issues), 0,
            f"Critical completeness issues found in {test_doctype}: {critical_issues}"
        )
        
        print(f"âœ… DocType completeness: {test_doctype} passed validation with {len(issues)} minor issues")


class TestPreCommitIntegration(unittest.TestCase):
    """Test pre-commit hook integration"""
    
    def test_pre_commit_hook_compatibility(self):
        """Test that standardization doesn't break pre-commit hooks"""
        # Check that pre-commit files can import standardized validators
        pre_commit_dir = Path("/home/frappe/frappe-bench/apps/verenigingen/scripts/validation")
        
        # Test files that might be used in pre-commit
        pre_commit_scripts = [
            'pre_commit_js_python_check.py',
            'validation_suite_runner.py'
        ]
        
        for script_name in pre_commit_scripts:
            script_path = pre_commit_dir / script_name
            if script_path.exists():
                try:
                    # Try to import/execute the script to check for syntax errors
                    with open(script_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Basic syntax check
                    compile(content, str(script_path), 'exec')
                    
                    print(f"âœ… Pre-commit compatibility: {script_name} syntax validated")
                    
                except SyntaxError as e:
                    self.fail(f"Syntax error in pre-commit script {script_name}: {e}")
                except Exception as e:
                    # Other errors might be expected (import issues in test env)
                    print(f"â„¹ï¸  Pre-commit script {script_name}: {e} (may be expected in test environment)")


def run_comprehensive_tests():
    """Run all comprehensive tests and provide summary"""
    print("ğŸ§ª Running Comprehensive DocType Loader Standardization Tests")
    print("=" * 80)
    
    # Create test suite
    test_suite = unittest.TestSuite()
    
    # Add all test classes
    test_classes = [
        TestDocTypeLoaderCore,
        TestValidatorStandardization, 
        TestRegressionDetection,
        TestFunctionalValidation,
        TestPreCommitIntegration
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # Run tests with detailed output
    runner = unittest.TextTestRunner(verbosity=2, stream=None)
    result = runner.run(test_suite)
    
    # Print summary
    print("\n" + "=" * 80)
    print("ğŸ Test Summary")
    print("=" * 80)
    
    total_tests = result.testsRun
    failures = len(result.failures)
    errors = len(result.errors)
    skipped = len(result.skipped) if hasattr(result, 'skipped') else 0
    passed = total_tests - failures - errors - skipped
    
    print(f"Total Tests: {total_tests}")
    print(f"âœ… Passed: {passed}")
    print(f"âŒ Failed: {failures}")
    print(f"ğŸš« Errors: {errors}")
    print(f"â­ï¸  Skipped: {skipped}")
    
    if failures > 0:
        print("\nğŸ”´ Failures:")
        for test, traceback in result.failures:
            print(f"  - {test}: {traceback.split('\\n')[-2] if traceback else 'Unknown failure'}")
    
    if errors > 0:
        print("\nğŸš« Errors:")
        for test, traceback in result.errors:
            print(f"  - {test}: {traceback.split('\\n')[-2] if traceback else 'Unknown error'}")
    
    success_rate = (passed / total_tests) * 100 if total_tests > 0 else 0
    print(f"\nğŸ“Š Success Rate: {success_rate:.1f}%")
    
    if success_rate >= 90:
        print("ğŸ‰ Standardization tests PASSED! Infrastructure is working correctly.")
        return True
    else:
        print("âš ï¸  Standardization tests show issues that need attention.")
        return False


if __name__ == "__main__":
    success = run_comprehensive_tests()
    exit(0 if success else 1)